{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the CernVM-FS tutorial! \u00b6 Scope \u00b6 describe scope here Intended audience \u00b6 describe intended audience here Prerequisites \u00b6 describe prerequisites here Practical information \u00b6 describe practical info here Tutorial contents \u00b6 Practical information Introduction to CernVM-FS Getting started Stratum-1 + proxies Publishing Advanced topics (sections indicated with (*) involve hands-on exercises) Contributors \u00b6 list contributors here Additional resources \u00b6 list additional resources here","title":"Home"},{"location":"#welcome-to-the-cernvm-fs-tutorial","text":"","title":"Welcome to the CernVM-FS tutorial!"},{"location":"#scope","text":"describe scope here","title":"Scope"},{"location":"#intended-audience","text":"describe intended audience here","title":"Intended audience"},{"location":"#prerequisites","text":"describe prerequisites here","title":"Prerequisites"},{"location":"#practical-information","text":"describe practical info here","title":"Practical information"},{"location":"#tutorial-contents","text":"Practical information Introduction to CernVM-FS Getting started Stratum-1 + proxies Publishing Advanced topics (sections indicated with (*) involve hands-on exercises)","title":"Tutorial contents"},{"location":"#contributors","text":"list contributors here","title":"Contributors"},{"location":"#additional-resources","text":"list additional resources here","title":"Additional resources"},{"location":"00_practical_info/","text":"Practical info \u00b6 Azure/AWS VMs Q&A: 30min in afternoon (4pm-4.30pm) or book a time slot (via Google Calendar)","title":"Practical information"},{"location":"00_practical_info/#practical-info","text":"Azure/AWS VMs Q&A: 30min in afternoon (4pm-4.30pm) or book a time slot (via Google Calendar)","title":"Practical info"},{"location":"01_introduction/","text":"Introduction to CernVM-FS \u00b6 Stratum-0 + create repo add \"hello world\" bash script client access hands-on homework","title":"Introduction to CernVM-FS"},{"location":"01_introduction/#introduction-to-cernvm-fs","text":"Stratum-0 + create repo add \"hello world\" bash script client access hands-on homework","title":"Introduction to CernVM-FS"},{"location":"02_getting_started/","text":"Getting started \u00b6 In order to get started with CernVM-FS, the first thing you need is a Stratum 0 server. This is the central server that hosts your repositories and makes it available to other machines. There can be only one Stratum 0 server for each repository, and from a security perspective it is usually recommended to restrict the access to this machine. We will look more into that later; for now, we are going to set up a Stratum 0, make a repository, and connect from a client machine directly to the Stratum 0. Set up the Stratum 0 \u00b6 Requirements \u00b6 Due to the scalable design of CernVM-FS, the host of your Stratum 0 server does not need a lot of resources in terms of CPU cores and memory; just a few cores and a few gigabytes of memory should suffice. Besides this, you need plenty of space to store the contents of your repository. By default, CernVM-FS uses /var/spool as scratch space while adding new files to the repository, and /srv/cvmfs as central repository storage location, but these locations can be modified in the CernVM-FS server settings later on. Furthermore, several (popular) Linux distributions are supported, see these requirements for a full list. We will only focus on CentOS in this tutorial. CernVM-FS also offers support for hosting the repository contents in S3 compatible storage, but for this tutorial we will focus on storing the files locally. For this we need an Apache server on the host, and port 80 should be open. Installation \u00b6 The installation of CernVM-FS is simple and only requires some packages to be installed. You can easily do this by adding the CernVM-FS repository and install the packages through your package manager: sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y epel-release sudo yum install -y cvmfs cvmfs-server Alternatively, you can download the packages from the CernVM-FS downloads page and install them package manually; note that you need both the client and server package on your Stratum 0. Start Apache \u00b6 Since the Stratum 0 is serving contents via HTTP, Apache needs to be running before we can make a repository. The Apache ( httpd ) package should have been installed already, as it is a dependency of the cvmfs-server package, so it can now be enabled (so that it always starts after a reboot) and started using: sudo systemctl enable httpd sudo systemctl start httpd Create a repository \u00b6 Now that all required packages have been installed, it is time to create a repository. In the simplest way, this can be done by running the following command, which will make $USER the owner of the repository: sudo cvmfs_server mkfs -o $USER repo.organization.tld The full repository name, here repo.organization.tld , resembles a DNS name, but the organization.tld domain does not necessarily have to exist. It is recommended to give all the repositories belonging to the same project or organization the same .organization.tld domain here. This makes the client configuration much easier, also in case new repositories will be added later on. Repository keys \u00b6 For each repository that you create, a set of keys will be generated in /etc/cvmfs/keys : - repo.organization.tld.crt - the repository\u2019s public key (encoded as X509 certificate); - repo.organization.tld.key - the repository's private key; - repo.organization.tld.masterkey - the repository's private master key; - repo.organization.tld.pub - repository\u2019s public key (RSA). The public key is the one that is needed by clients in order to access the repository; we will need this later on. The master key is used to sign a whitelist of known publisher certificates; this whitelist is, by default, valid for 30 days, so the signing has to be done regularly. For now we are going to use one master key per repository, but in practice it is recommended to use the same master key for all repositories under a single domain, so that clients only need a single public key to access all repositories under this domain. We will explain this in more detail in the advanced section. TODO: DO WE WANT TO EXPLAIN THIS? Add some files to the repository \u00b6 A new repository automatically gets a file new_repository in its root ( /cvmfs/repo.organization.tld ). You can add more files by starting and publishing a transaction, which will be explained in more detail in a later section. For now it is enough to just run the following commands as root: MY_REPO_NAME = repo.organization.tld sudo cvmfs_server transaction ${ MY_REPO_NAME } # Now make some changes in /cvmfs/${MY_REPO_NAME}, # e.g. by adding files or directories. # If you made $USER the owner of the repository, you can do this without sudo. sudo cvmfs_server publish ${ MY_REPO_NAME } Cronjob for resigning the whitelist \u00b6 Each CernVM-FS repository has a whitelist containing fingerprints of certificates that are allowed to sign the repository. This whitelist has an expiration time of, by default, 30 days. This means that you regularly have to resign the whitelist. There are several ways to do this, see for instance the page about master keys in the documentation. If you just keep the master key on our Stratum 0 sever, you can set up a simple cronjob for resigning the whitelist. For instance, make a file /etc/cron.d/cvmfs_resign with the following content to do this every Monday at 11:00: 0 11 * * 1 root /usr/bin/cvmfs_server resign repo.organization.tld Remove a repository \u00b6 An existing repository can be removed by running: sudo cvmfs_server rmfs repo.organization.tld Set up a client \u00b6 Accessing CernVM-FS repositories on a client machine involves three steps: installing the CernVM-FS client package, adding some configuration files for the repository you want to connect to, and finally run a CernVM-FS setup procedure that will mount the repository. Since the client is going to pull in files over an HTTP connection, you need sufficient space for storing a local cache on the client machine. You can define the maximum size of your cache in the settings; the larger your cache is, the less often you have to pull in files again, and the faster your applications will start. Note that you can add more cache layers by adding a proxy nearby your client; this will be covered in a later section. Installation \u00b6 The installation is the same as for the Stratum 0, except that you only need the cvmfs package: sudo yum install https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs Configuration \u00b6 Many popular/large organizations hosting CernVM-FS repositories offer a client package that you can install to do most of the configuration. For our repository, we are going to do this manually. All required configuration files will have to be stored somewhere under /etc/cvmfs . We will discuss them one by one, where we use repo.organization.tld as repository name, and hence organization.tld as domain. /etc/cvmfs/keys/organization.tld/repo.organization.tld.pub \u00b6 This file contains the public key of the repository you want to access. You can copy this file from your Stratum 0 server, where it should be stored at the same location. /etc/cvmfs/config.d/repo.organization.tld.conf \u00b6 This file contains the main configuration for the repository you want to access, which should minimally contain the URL(s) of the Stratum 1 servers and the location of the public key. Because we do not have a Stratum 1 server yet, we are going to (mis)use our Stratum 0 as a Stratum 1. You should not do this in production! A typical, minimal configuration should look as follows: CVMFS_SERVER_URL=\"http://your-stratum0/cvmfs/@fqrn@\" CVMFS_PUBLIC_KEY=\"/etc/cvmfs/keys/organization.tld/repo.organization.tld.pub\" Note that the CVMFS_SERVER_URL should have the /cvmfs/@fqrn ; the last part will automatically be replaced by the full name of your repository. /etc/cvmfs/default.local \u00b6 This file can be used for setting or overriding settings that are specific to your client machine. One required parameter is CVMFS_HTTP_PROXY , which should point to your local proxy that serves as a cache between your client(s) and the Stratum 1 server(s). Since we do not have a proxy yet, we are setting this to DIRECT , meaning that we connect directly to the Stratum 1: CVMFS_HTTP_PROXY=DIRECT You can also use this file to set a maximum size (in megabytes) for the cache, for instance: CVMFS_QUOTA_LIMIT=50000 Mount the repositories \u00b6 When your configuration is complete, you can run the following command as root to mount the repository: sudo cvmfs_config setup This should not return any error message. Browse the repository \u00b6 Finally, we can try to access our repository on the client machine. Note that CernVM-FS uses autofs , which means that you may not see the repository when you do ls /cvmfs . Your repository will only be actually mounted when you access it, and may be unmounted after not using it for a while. So, the following should work and show the contents of your repository: ls /cvmfs/repo.organization.tld Exercise \u00b6 Set up your own Stratum 0 on a virtual machine. Create a repository with a suitable name ( name.domain.tld ). Add a simple bash script to the repository, which, for instance, prints Hello world! . Install and configure the CernVM-FS client on another virtual machine, using the Stratum 0 as your Stratum 1 for now. Try to access your repository and run your bash script on the client.","title":"Getting started"},{"location":"02_getting_started/#getting-started","text":"In order to get started with CernVM-FS, the first thing you need is a Stratum 0 server. This is the central server that hosts your repositories and makes it available to other machines. There can be only one Stratum 0 server for each repository, and from a security perspective it is usually recommended to restrict the access to this machine. We will look more into that later; for now, we are going to set up a Stratum 0, make a repository, and connect from a client machine directly to the Stratum 0.","title":"Getting started"},{"location":"02_getting_started/#set-up-the-stratum-0","text":"","title":"Set up the Stratum 0"},{"location":"02_getting_started/#requirements","text":"Due to the scalable design of CernVM-FS, the host of your Stratum 0 server does not need a lot of resources in terms of CPU cores and memory; just a few cores and a few gigabytes of memory should suffice. Besides this, you need plenty of space to store the contents of your repository. By default, CernVM-FS uses /var/spool as scratch space while adding new files to the repository, and /srv/cvmfs as central repository storage location, but these locations can be modified in the CernVM-FS server settings later on. Furthermore, several (popular) Linux distributions are supported, see these requirements for a full list. We will only focus on CentOS in this tutorial. CernVM-FS also offers support for hosting the repository contents in S3 compatible storage, but for this tutorial we will focus on storing the files locally. For this we need an Apache server on the host, and port 80 should be open.","title":"Requirements"},{"location":"02_getting_started/#installation","text":"The installation of CernVM-FS is simple and only requires some packages to be installed. You can easily do this by adding the CernVM-FS repository and install the packages through your package manager: sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y epel-release sudo yum install -y cvmfs cvmfs-server Alternatively, you can download the packages from the CernVM-FS downloads page and install them package manually; note that you need both the client and server package on your Stratum 0.","title":"Installation"},{"location":"02_getting_started/#start-apache","text":"Since the Stratum 0 is serving contents via HTTP, Apache needs to be running before we can make a repository. The Apache ( httpd ) package should have been installed already, as it is a dependency of the cvmfs-server package, so it can now be enabled (so that it always starts after a reboot) and started using: sudo systemctl enable httpd sudo systemctl start httpd","title":"Start Apache"},{"location":"02_getting_started/#create-a-repository","text":"Now that all required packages have been installed, it is time to create a repository. In the simplest way, this can be done by running the following command, which will make $USER the owner of the repository: sudo cvmfs_server mkfs -o $USER repo.organization.tld The full repository name, here repo.organization.tld , resembles a DNS name, but the organization.tld domain does not necessarily have to exist. It is recommended to give all the repositories belonging to the same project or organization the same .organization.tld domain here. This makes the client configuration much easier, also in case new repositories will be added later on.","title":"Create a repository"},{"location":"02_getting_started/#repository-keys","text":"For each repository that you create, a set of keys will be generated in /etc/cvmfs/keys : - repo.organization.tld.crt - the repository\u2019s public key (encoded as X509 certificate); - repo.organization.tld.key - the repository's private key; - repo.organization.tld.masterkey - the repository's private master key; - repo.organization.tld.pub - repository\u2019s public key (RSA). The public key is the one that is needed by clients in order to access the repository; we will need this later on. The master key is used to sign a whitelist of known publisher certificates; this whitelist is, by default, valid for 30 days, so the signing has to be done regularly. For now we are going to use one master key per repository, but in practice it is recommended to use the same master key for all repositories under a single domain, so that clients only need a single public key to access all repositories under this domain. We will explain this in more detail in the advanced section. TODO: DO WE WANT TO EXPLAIN THIS?","title":"Repository keys"},{"location":"02_getting_started/#add-some-files-to-the-repository","text":"A new repository automatically gets a file new_repository in its root ( /cvmfs/repo.organization.tld ). You can add more files by starting and publishing a transaction, which will be explained in more detail in a later section. For now it is enough to just run the following commands as root: MY_REPO_NAME = repo.organization.tld sudo cvmfs_server transaction ${ MY_REPO_NAME } # Now make some changes in /cvmfs/${MY_REPO_NAME}, # e.g. by adding files or directories. # If you made $USER the owner of the repository, you can do this without sudo. sudo cvmfs_server publish ${ MY_REPO_NAME }","title":"Add some files to the repository"},{"location":"02_getting_started/#cronjob-for-resigning-the-whitelist","text":"Each CernVM-FS repository has a whitelist containing fingerprints of certificates that are allowed to sign the repository. This whitelist has an expiration time of, by default, 30 days. This means that you regularly have to resign the whitelist. There are several ways to do this, see for instance the page about master keys in the documentation. If you just keep the master key on our Stratum 0 sever, you can set up a simple cronjob for resigning the whitelist. For instance, make a file /etc/cron.d/cvmfs_resign with the following content to do this every Monday at 11:00: 0 11 * * 1 root /usr/bin/cvmfs_server resign repo.organization.tld","title":"Cronjob for resigning the whitelist"},{"location":"02_getting_started/#remove-a-repository","text":"An existing repository can be removed by running: sudo cvmfs_server rmfs repo.organization.tld","title":"Remove a repository"},{"location":"02_getting_started/#set-up-a-client","text":"Accessing CernVM-FS repositories on a client machine involves three steps: installing the CernVM-FS client package, adding some configuration files for the repository you want to connect to, and finally run a CernVM-FS setup procedure that will mount the repository. Since the client is going to pull in files over an HTTP connection, you need sufficient space for storing a local cache on the client machine. You can define the maximum size of your cache in the settings; the larger your cache is, the less often you have to pull in files again, and the faster your applications will start. Note that you can add more cache layers by adding a proxy nearby your client; this will be covered in a later section.","title":"Set up a client"},{"location":"02_getting_started/#installation_1","text":"The installation is the same as for the Stratum 0, except that you only need the cvmfs package: sudo yum install https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs","title":"Installation"},{"location":"02_getting_started/#configuration","text":"Many popular/large organizations hosting CernVM-FS repositories offer a client package that you can install to do most of the configuration. For our repository, we are going to do this manually. All required configuration files will have to be stored somewhere under /etc/cvmfs . We will discuss them one by one, where we use repo.organization.tld as repository name, and hence organization.tld as domain.","title":"Configuration"},{"location":"02_getting_started/#etccvmfskeysorganizationtldrepoorganizationtldpub","text":"This file contains the public key of the repository you want to access. You can copy this file from your Stratum 0 server, where it should be stored at the same location.","title":"/etc/cvmfs/keys/organization.tld/repo.organization.tld.pub"},{"location":"02_getting_started/#etccvmfsconfigdrepoorganizationtldconf","text":"This file contains the main configuration for the repository you want to access, which should minimally contain the URL(s) of the Stratum 1 servers and the location of the public key. Because we do not have a Stratum 1 server yet, we are going to (mis)use our Stratum 0 as a Stratum 1. You should not do this in production! A typical, minimal configuration should look as follows: CVMFS_SERVER_URL=\"http://your-stratum0/cvmfs/@fqrn@\" CVMFS_PUBLIC_KEY=\"/etc/cvmfs/keys/organization.tld/repo.organization.tld.pub\" Note that the CVMFS_SERVER_URL should have the /cvmfs/@fqrn ; the last part will automatically be replaced by the full name of your repository.","title":"/etc/cvmfs/config.d/repo.organization.tld.conf"},{"location":"02_getting_started/#etccvmfsdefaultlocal","text":"This file can be used for setting or overriding settings that are specific to your client machine. One required parameter is CVMFS_HTTP_PROXY , which should point to your local proxy that serves as a cache between your client(s) and the Stratum 1 server(s). Since we do not have a proxy yet, we are setting this to DIRECT , meaning that we connect directly to the Stratum 1: CVMFS_HTTP_PROXY=DIRECT You can also use this file to set a maximum size (in megabytes) for the cache, for instance: CVMFS_QUOTA_LIMIT=50000","title":"/etc/cvmfs/default.local"},{"location":"02_getting_started/#mount-the-repositories","text":"When your configuration is complete, you can run the following command as root to mount the repository: sudo cvmfs_config setup This should not return any error message.","title":"Mount the repositories"},{"location":"02_getting_started/#browse-the-repository","text":"Finally, we can try to access our repository on the client machine. Note that CernVM-FS uses autofs , which means that you may not see the repository when you do ls /cvmfs . Your repository will only be actually mounted when you access it, and may be unmounted after not using it for a while. So, the following should work and show the contents of your repository: ls /cvmfs/repo.organization.tld","title":"Browse the repository"},{"location":"02_getting_started/#exercise","text":"Set up your own Stratum 0 on a virtual machine. Create a repository with a suitable name ( name.domain.tld ). Add a simple bash script to the repository, which, for instance, prints Hello world! . Install and configure the CernVM-FS client on another virtual machine, using the Stratum 0 as your Stratum 1 for now. Try to access your repository and run your bash script on the client.","title":"Exercise"},{"location":"03_stratum1_proxies/","text":"Stratum 1 and proxies \u00b6 short summary previous session Stratum-1 proxy hands-on homework In the previous section we have set up a Stratum 0 server and a client that directly connects to the Stratum 0. Although this worked fine, this setup is neither scalable, nor very reliable, nor secure: it is a single point of failure, too open in terms of connectivity, and it will have to serve all possible clients on its own. Therefore, this section will show how all these points can be addressed by adding one or more Stratum 1 servers and caching proxy servers. A Stratum 1 is a replica server that keeps mirrors of the repositories served by a Stratum 0. It is basically a web server that periodically synchronizes the contents of the repositories, and, in contrast to a Stratum 0 server, you can have multiple Stratum 1 servers. It is recommended to have several ones that are geographically distributed, so that clients always have a nearby Stratum 1 server. How many you need mostly depends on the distribution and number of clients, but often a few is already sufficient. More scalability can be added with proxies, which we will discuss later in this section. INSERT IMAGE OF CVMFS INFRA HERE Set up a Stratum 1 server \u00b6 Requirements \u00b6 A Stratum 1 servers has similar requirements as a Stratum 1. In addition to port 80, also port 8000 has to be accessible for a Stratum 1. Furthermore, you need a (free) license key for Maxmind's Geo API , which you can obtain by signing up for an account . Installation \u00b6 For the Stratum 1 you need to install the following packages: sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y epel-release sudo yum install -y cvmfs-server sudo yum install -y mod_wsgi sudo yum install -y squid Apache and Squid \u00b6 port 8080 locally? -> Listen 127.0.0.1:8080 sudo systemctl enable httpd sudo systemctl start httpd /etc/squid/squid.conf: http_port 80 accel http_port 8000 accel http_access allow all cache_peer localhost parent 8080 0 no-query originserver acl CVMFSAPI urlpath_regex ^/cvmfs/[^/]*/api/ cache deny !CVMFSAPI cache_mem 256 MB ulimit -n 8192 ? start squid DNS cache? \u00b6 Skip this for now and point to the documentation? Register the Stratum 1 \u00b6 echo 'CVMFS_GEO_LICENSE_KEY=YOUR_KEY' >> /etc/cvmfs/server.local chmod 600 /etc/cvmfs/server.local Make the first snapshot \u00b6 cvmfs_server snapshot repo.organization.tld Make cronjobs \u00b6 /etc/logrotate.d/cvmfs does not exist! To prevent this error message, create the file or use -n option. Suggested content: /var/log/cvmfs/*.log { weekly missingok notifempty } $ cat /etc/cron.d/cvmfs_stratum1_snapshot */5 * * * * root output=$(/usr/bin/cvmfs_server snapshot -a -i 2>&1) || echo \"$output\" $ cat /etc/cron.d/cvmfs_geoip_db_update 4 2 2 * * root /usr/bin/cvmfs_server update-geodb Set up a proxy \u00b6 Client configuration \u00b6 Homework \u00b6","title":"Stratum-1 + proxies"},{"location":"03_stratum1_proxies/#stratum-1-and-proxies","text":"short summary previous session Stratum-1 proxy hands-on homework In the previous section we have set up a Stratum 0 server and a client that directly connects to the Stratum 0. Although this worked fine, this setup is neither scalable, nor very reliable, nor secure: it is a single point of failure, too open in terms of connectivity, and it will have to serve all possible clients on its own. Therefore, this section will show how all these points can be addressed by adding one or more Stratum 1 servers and caching proxy servers. A Stratum 1 is a replica server that keeps mirrors of the repositories served by a Stratum 0. It is basically a web server that periodically synchronizes the contents of the repositories, and, in contrast to a Stratum 0 server, you can have multiple Stratum 1 servers. It is recommended to have several ones that are geographically distributed, so that clients always have a nearby Stratum 1 server. How many you need mostly depends on the distribution and number of clients, but often a few is already sufficient. More scalability can be added with proxies, which we will discuss later in this section. INSERT IMAGE OF CVMFS INFRA HERE","title":"Stratum 1 and proxies"},{"location":"03_stratum1_proxies/#set-up-a-stratum-1-server","text":"","title":"Set up a Stratum 1 server"},{"location":"03_stratum1_proxies/#requirements","text":"A Stratum 1 servers has similar requirements as a Stratum 1. In addition to port 80, also port 8000 has to be accessible for a Stratum 1. Furthermore, you need a (free) license key for Maxmind's Geo API , which you can obtain by signing up for an account .","title":"Requirements"},{"location":"03_stratum1_proxies/#installation","text":"For the Stratum 1 you need to install the following packages: sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y epel-release sudo yum install -y cvmfs-server sudo yum install -y mod_wsgi sudo yum install -y squid","title":"Installation"},{"location":"03_stratum1_proxies/#apache-and-squid","text":"port 8080 locally? -> Listen 127.0.0.1:8080 sudo systemctl enable httpd sudo systemctl start httpd /etc/squid/squid.conf: http_port 80 accel http_port 8000 accel http_access allow all cache_peer localhost parent 8080 0 no-query originserver acl CVMFSAPI urlpath_regex ^/cvmfs/[^/]*/api/ cache deny !CVMFSAPI cache_mem 256 MB ulimit -n 8192 ? start squid","title":"Apache and Squid"},{"location":"03_stratum1_proxies/#dns-cache","text":"Skip this for now and point to the documentation?","title":"DNS cache?"},{"location":"03_stratum1_proxies/#register-the-stratum-1","text":"echo 'CVMFS_GEO_LICENSE_KEY=YOUR_KEY' >> /etc/cvmfs/server.local chmod 600 /etc/cvmfs/server.local","title":"Register the Stratum 1"},{"location":"03_stratum1_proxies/#make-the-first-snapshot","text":"cvmfs_server snapshot repo.organization.tld","title":"Make the first snapshot"},{"location":"03_stratum1_proxies/#make-cronjobs","text":"/etc/logrotate.d/cvmfs does not exist! To prevent this error message, create the file or use -n option. Suggested content: /var/log/cvmfs/*.log { weekly missingok notifempty } $ cat /etc/cron.d/cvmfs_stratum1_snapshot */5 * * * * root output=$(/usr/bin/cvmfs_server snapshot -a -i 2>&1) || echo \"$output\" $ cat /etc/cron.d/cvmfs_geoip_db_update 4 2 2 * * root /usr/bin/cvmfs_server update-geodb","title":"Make cronjobs"},{"location":"03_stratum1_proxies/#set-up-a-proxy","text":"","title":"Set up a proxy"},{"location":"03_stratum1_proxies/#client-configuration","text":"","title":"Client configuration"},{"location":"03_stratum1_proxies/#homework","text":"","title":"Homework"},{"location":"04_publishing/","text":"Publishing \u00b6 nested catalogs via Stratum-0 hands-on: publish prepared software stack","title":"Publishing"},{"location":"04_publishing/#publishing","text":"nested catalogs via Stratum-0 hands-on: publish prepared software stack","title":"Publishing"},{"location":"05_advanced/","text":"Advanced topics \u00b6 gateway-publisher (with warnings) installing these takes time prepared: 1 gateway + 2 publishers demo only (no hands-on) exploding containers + use via Singularity demo","title":"Advanced topics"},{"location":"05_advanced/#advanced-topics","text":"gateway-publisher (with warnings) installing these takes time prepared: 1 gateway + 2 publishers demo only (no hands-on) exploding containers + use via Singularity demo","title":"Advanced topics"}]}