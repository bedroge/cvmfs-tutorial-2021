{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the CernVM-FS tutorial! \u00b6 Scope \u00b6 describe scope here Intended audience \u00b6 describe intended audience here Prerequisites \u00b6 describe prerequisites here Practical information \u00b6 describe practical info here Tutorial contents \u00b6 Practical information Introduction to CernVM-FS Getting started Stratum-1 + proxies Publishing Advanced topics (sections indicated with (*) involve hands-on exercises) Contributors \u00b6 list contributors here Additional resources \u00b6 list additional resources here","title":"Home"},{"location":"#welcome-to-the-cernvm-fs-tutorial","text":"","title":"Welcome to the CernVM-FS tutorial!"},{"location":"#scope","text":"describe scope here","title":"Scope"},{"location":"#intended-audience","text":"describe intended audience here","title":"Intended audience"},{"location":"#prerequisites","text":"describe prerequisites here","title":"Prerequisites"},{"location":"#practical-information","text":"describe practical info here","title":"Practical information"},{"location":"#tutorial-contents","text":"Practical information Introduction to CernVM-FS Getting started Stratum-1 + proxies Publishing Advanced topics (sections indicated with (*) involve hands-on exercises)","title":"Tutorial contents"},{"location":"#contributors","text":"list contributors here","title":"Contributors"},{"location":"#additional-resources","text":"list additional resources here","title":"Additional resources"},{"location":"00_practical_info/","text":"Practical info \u00b6 Azure/AWS VMs Q&A: 30min in afternoon (4pm-4.30pm) or book a time slot (via Google Calendar)","title":"Practical information"},{"location":"00_practical_info/#practical-info","text":"Azure/AWS VMs Q&A: 30min in afternoon (4pm-4.30pm) or book a time slot (via Google Calendar)","title":"Practical info"},{"location":"01_introduction/","text":"Introduction to CernVM-FS \u00b6 Stratum-0 + create repo add \"hello world\" bash script client access hands-on homework","title":"Introduction to CernVM-FS"},{"location":"01_introduction/#introduction-to-cernvm-fs","text":"Stratum-0 + create repo add \"hello world\" bash script client access hands-on homework","title":"Introduction to CernVM-FS"},{"location":"02_getting_started/","text":"Getting started \u00b6 In order to get started with CernVM-FS, the first thing you need is a Stratum 0 server. This is the central server that hosts your repositories and makes it available to other machines. There can be only one Stratum 0 server for each repository, and from a security perspective it is usually recommended to restrict the access to this machine. We will look more into that later; for now, we are going to set up a Stratum 0, make a repository, and connect from a client machine directly to the Stratum 0. Set up the Stratum 0 \u00b6 Requirements \u00b6 Due to the scalable design of CernVM-FS, the host of your Stratum 0 server does not need a lot of resources in terms of CPU cores and memory; just a few cores and a few gigabytes of memory should suffice. Besides this, you need plenty of space to store the contents of your repository. By default, CernVM-FS uses /var/spool as scratch space while adding new files to the repository, and /srv/cvmfs as central repository storage location, but these locations can be modified in the CernVM-FS server settings later on. Furthermore, several (popular) Linux distributions are supported, see these requirements for a full list. We will only focus on CentOS in this tutorial. CernVM-FS also offers support for hosting the repository contents in S3 compatible storage, but for this tutorial we will focus on storing the files locally. For this we need an Apache server on the host, and port 80 should be open. Installation \u00b6 The installation of CernVM-FS is simple and only requires some packages to be installed. You can easily do this by adding the CernVM-FS repository and install the packages through your package manager: sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y epel-release sudo yum install -y cvmfs cvmfs-server Alternatively, you can download the packages from the CernVM-FS downloads page and install them package manually; note that you need both the client and server package on your Stratum 0. Start Apache \u00b6 Since the Stratum 0 is serving contents via HTTP, Apache needs to be running before we can make a repository. The Apache ( httpd ) package should have been installed already, as it is a dependency of the cvmfs-server package, so it can now be enabled (so that it always starts after a reboot) and started using: sudo systemctl enable httpd sudo systemctl start httpd Create a repository \u00b6 Now that all required packages have been installed, it is time to create a repository. In the simplest way, this can be done by running the following command, which will make $USER the owner of the repository: sudo cvmfs_server mkfs -o $USER repo.organization.tld The full repository name, here repo.organization.tld , resembles a DNS name, but the organization.tld domain does not necessarily have to exist. It is recommended to give all the repositories belonging to the same project or organization the same .organization.tld domain here. This makes the client configuration much easier, also in case new repositories will be added later on. Repository keys \u00b6 For each repository that you create, a set of keys will be generated in /etc/cvmfs/keys : - repo.organization.tld.crt - the repository\u2019s public key (encoded as X509 certificate); - repo.organization.tld.key - the repository's private key; - repo.organization.tld.masterkey - the repository's private master key; - repo.organization.tld.pub - repository\u2019s public key (RSA). The public key is the one that is needed by clients in order to access the repository; we will need this later on. The master key is used to sign a whitelist of known publisher certificates; this whitelist is, by default, valid for 30 days, so the signing has to be done regularly. For now we are going to use one master key per repository, but in practice it is recommended to use the same master key for all repositories under a single domain, so that clients only need a single public key to access all repositories under this domain. We will explain this in more detail in the advanced section. TODO: DO WE WANT TO EXPLAIN THIS? Add some files to the repository \u00b6 A new repository automatically gets a file new_repository in its root ( /cvmfs/repo.organization.tld ). You can add more files by starting and publishing a transaction, which will be explained in more detail in a later section. For now it is enough to just run the following commands as root: MY_REPO_NAME = repo.organization.tld sudo cvmfs_server transaction ${ MY_REPO_NAME } # Now make some changes in /cvmfs/${MY_REPO_NAME}, # e.g. by adding files or directories. # If you made $USER the owner of the repository, you can do this without sudo. sudo cvmfs_server publish ${ MY_REPO_NAME } Cronjob for resigning the whitelist \u00b6 Each CernVM-FS repository has a whitelist containing fingerprints of certificates that are allowed to sign the repository. This whitelist has an expiration time of, by default, 30 days. This means that you regularly have to resign the whitelist. There are several ways to do this, see for instance the page about master keys in the documentation. If you just keep the master key on our Stratum 0 sever, you can set up a simple cronjob for resigning the whitelist. For instance, make a file /etc/cron.d/cvmfs_resign with the following content to do this every Monday at 11:00: 0 11 * * 1 root /usr/bin/cvmfs_server resign repo.organization.tld Remove a repository \u00b6 An existing repository can be removed by running: sudo cvmfs_server rmfs repo.organization.tld Set up a client \u00b6 Accessing CernVM-FS repositories on a client machine involves three steps: installing the CernVM-FS client package, adding some configuration files for the repository you want to connect to, and finally run a CernVM-FS setup procedure that will mount the repository. Since the client is going to pull in files over an HTTP connection, you need sufficient space for storing a local cache on the client machine. You can define the maximum size of your cache in the settings; the larger your cache is, the less often you have to pull in files again, and the faster your applications will start. Note that you can add more cache layers by adding a proxy nearby your client; this will be covered in a later section. Installation \u00b6 The installation is the same as for the Stratum 0, except that you only need the cvmfs package: sudo yum install https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs Configuration \u00b6 Many popular/large organizations hosting CernVM-FS repositories offer a client package that you can install to do most of the configuration. For our repository, we are going to do this manually. All required configuration files will have to be stored somewhere under /etc/cvmfs . We will discuss them one by one, where we use repo.organization.tld as repository name, and hence organization.tld as domain. /etc/cvmfs/keys/organization.tld/repo.organization.tld.pub \u00b6 This file contains the public key of the repository you want to access. You can copy this file from your Stratum 0 server, where it should be stored at the same location. /etc/cvmfs/config.d/repo.organization.tld.conf \u00b6 This file contains the main configuration for the repository you want to access, which should minimally contain the URL(s) of the Stratum 1 servers and the location of the public key. Because we do not have a Stratum 1 server yet, we are going to (mis)use our Stratum 0 as a Stratum 1. You should not do this in production! A typical, minimal configuration should look as follows: CVMFS_SERVER_URL=\"http://your-stratum0/cvmfs/@fqrn@\" CVMFS_PUBLIC_KEY=\"/etc/cvmfs/keys/organization.tld/repo.organization.tld.pub\" Note that the CVMFS_SERVER_URL should have the /cvmfs/@fqrn ; the last part will automatically be replaced by the full name of your repository. /etc/cvmfs/default.local \u00b6 This file can be used for setting or overriding settings that are specific to your client machine. One required parameter is CVMFS_HTTP_PROXY , which should point to your local proxy that serves as a cache between your client(s) and the Stratum 1 server(s). Since we do not have a proxy yet, we are setting this to DIRECT , meaning that we connect directly to the Stratum 1: CVMFS_HTTP_PROXY=DIRECT You can also use this file to set a maximum size (in megabytes) for the cache, for instance: CVMFS_QUOTA_LIMIT=50000 Mount the repositories \u00b6 When your configuration is complete, you can run the following command as root to mount the repository: sudo cvmfs_config setup This should not return any error message. Browse the repository \u00b6 Finally, we can try to access our repository on the client machine. Note that CernVM-FS uses autofs , which means that you may not see the repository when you do ls /cvmfs . Your repository will only be actually mounted when you access it, and may be unmounted after not using it for a while. So, the following should work and show the contents of your repository: ls /cvmfs/repo.organization.tld Exercise \u00b6 Set up your own Stratum 0 on a virtual machine. Create a repository with a suitable name ( name.domain.tld ). Add a simple bash script to the repository, which, for instance, prints Hello world! . Install and configure the CernVM-FS client on another virtual machine, using the Stratum 0 as your Stratum 1 for now. Try to access your repository and run your bash script on the client.","title":"Getting started"},{"location":"02_getting_started/#getting-started","text":"In order to get started with CernVM-FS, the first thing you need is a Stratum 0 server. This is the central server that hosts your repositories and makes it available to other machines. There can be only one Stratum 0 server for each repository, and from a security perspective it is usually recommended to restrict the access to this machine. We will look more into that later; for now, we are going to set up a Stratum 0, make a repository, and connect from a client machine directly to the Stratum 0.","title":"Getting started"},{"location":"02_getting_started/#set-up-the-stratum-0","text":"","title":"Set up the Stratum 0"},{"location":"02_getting_started/#requirements","text":"Due to the scalable design of CernVM-FS, the host of your Stratum 0 server does not need a lot of resources in terms of CPU cores and memory; just a few cores and a few gigabytes of memory should suffice. Besides this, you need plenty of space to store the contents of your repository. By default, CernVM-FS uses /var/spool as scratch space while adding new files to the repository, and /srv/cvmfs as central repository storage location, but these locations can be modified in the CernVM-FS server settings later on. Furthermore, several (popular) Linux distributions are supported, see these requirements for a full list. We will only focus on CentOS in this tutorial. CernVM-FS also offers support for hosting the repository contents in S3 compatible storage, but for this tutorial we will focus on storing the files locally. For this we need an Apache server on the host, and port 80 should be open.","title":"Requirements"},{"location":"02_getting_started/#installation","text":"The installation of CernVM-FS is simple and only requires some packages to be installed. You can easily do this by adding the CernVM-FS repository and install the packages through your package manager: sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y epel-release sudo yum install -y cvmfs cvmfs-server Alternatively, you can download the packages from the CernVM-FS downloads page and install them package manually; note that you need both the client and server package on your Stratum 0.","title":"Installation"},{"location":"02_getting_started/#start-apache","text":"Since the Stratum 0 is serving contents via HTTP, Apache needs to be running before we can make a repository. The Apache ( httpd ) package should have been installed already, as it is a dependency of the cvmfs-server package, so it can now be enabled (so that it always starts after a reboot) and started using: sudo systemctl enable httpd sudo systemctl start httpd","title":"Start Apache"},{"location":"02_getting_started/#create-a-repository","text":"Now that all required packages have been installed, it is time to create a repository. In the simplest way, this can be done by running the following command, which will make $USER the owner of the repository: sudo cvmfs_server mkfs -o $USER repo.organization.tld The full repository name, here repo.organization.tld , resembles a DNS name, but the organization.tld domain does not necessarily have to exist. It is recommended to give all the repositories belonging to the same project or organization the same .organization.tld domain here. This makes the client configuration much easier, also in case new repositories will be added later on.","title":"Create a repository"},{"location":"02_getting_started/#repository-keys","text":"For each repository that you create, a set of keys will be generated in /etc/cvmfs/keys : - repo.organization.tld.crt - the repository\u2019s public key (encoded as X509 certificate); - repo.organization.tld.key - the repository's private key; - repo.organization.tld.masterkey - the repository's private master key; - repo.organization.tld.pub - repository\u2019s public key (RSA). The public key is the one that is needed by clients in order to access the repository; we will need this later on. The master key is used to sign a whitelist of known publisher certificates; this whitelist is, by default, valid for 30 days, so the signing has to be done regularly. For now we are going to use one master key per repository, but in practice it is recommended to use the same master key for all repositories under a single domain, so that clients only need a single public key to access all repositories under this domain. We will explain this in more detail in the advanced section. TODO: DO WE WANT TO EXPLAIN THIS?","title":"Repository keys"},{"location":"02_getting_started/#add-some-files-to-the-repository","text":"A new repository automatically gets a file new_repository in its root ( /cvmfs/repo.organization.tld ). You can add more files by starting and publishing a transaction, which will be explained in more detail in a later section. For now it is enough to just run the following commands as root: MY_REPO_NAME = repo.organization.tld sudo cvmfs_server transaction ${ MY_REPO_NAME } # Now make some changes in /cvmfs/${MY_REPO_NAME}, # e.g. by adding files or directories. # If you made $USER the owner of the repository, you can do this without sudo. sudo cvmfs_server publish ${ MY_REPO_NAME }","title":"Add some files to the repository"},{"location":"02_getting_started/#cronjob-for-resigning-the-whitelist","text":"Each CernVM-FS repository has a whitelist containing fingerprints of certificates that are allowed to sign the repository. This whitelist has an expiration time of, by default, 30 days. This means that you regularly have to resign the whitelist. There are several ways to do this, see for instance the page about master keys in the documentation. If you just keep the master key on our Stratum 0 sever, you can set up a simple cronjob for resigning the whitelist. For instance, make a file /etc/cron.d/cvmfs_resign with the following content to do this every Monday at 11:00: 0 11 * * 1 root /usr/bin/cvmfs_server resign repo.organization.tld","title":"Cronjob for resigning the whitelist"},{"location":"02_getting_started/#remove-a-repository","text":"An existing repository can be removed by running: sudo cvmfs_server rmfs repo.organization.tld","title":"Remove a repository"},{"location":"02_getting_started/#set-up-a-client","text":"Accessing CernVM-FS repositories on a client machine involves three steps: installing the CernVM-FS client package, adding some configuration files for the repository you want to connect to, and finally run a CernVM-FS setup procedure that will mount the repository. Since the client is going to pull in files over an HTTP connection, you need sufficient space for storing a local cache on the client machine. You can define the maximum size of your cache in the settings; the larger your cache is, the less often you have to pull in files again, and the faster your applications will start. Note that you can add more cache layers by adding a proxy nearby your client; this will be covered in a later section.","title":"Set up a client"},{"location":"02_getting_started/#installation_1","text":"The installation is the same as for the Stratum 0, except that you only need the cvmfs package: sudo yum install https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y cvmfs","title":"Installation"},{"location":"02_getting_started/#configuration","text":"Many popular/large organizations hosting CernVM-FS repositories offer a client package that you can install to do most of the configuration. For our repository, we are going to do this manually. All required configuration files will have to be stored somewhere under /etc/cvmfs . We will discuss them one by one, where we use repo.organization.tld as repository name, and hence organization.tld as domain.","title":"Configuration"},{"location":"02_getting_started/#etccvmfskeysorganizationtldrepoorganizationtldpub","text":"This file contains the public key of the repository you want to access. You can copy this file from your Stratum 0 server, where it should be stored at the same location.","title":"/etc/cvmfs/keys/organization.tld/repo.organization.tld.pub"},{"location":"02_getting_started/#etccvmfsconfigdrepoorganizationtldconf","text":"This file contains the main configuration for the repository you want to access, which should minimally contain the URL(s) of the Stratum 1 servers and the location of the public key. Because we do not have a Stratum 1 server yet, we are going to (mis)use our Stratum 0 as a Stratum 1. You should not do this in production! A typical, minimal configuration should look as follows: CVMFS_SERVER_URL=\"http://your-stratum0/cvmfs/@fqrn@\" CVMFS_PUBLIC_KEY=\"/etc/cvmfs/keys/organization.tld/repo.organization.tld.pub\" Note that the CVMFS_SERVER_URL should have the /cvmfs/@fqrn ; the last part will automatically be replaced by the full name of your repository.","title":"/etc/cvmfs/config.d/repo.organization.tld.conf"},{"location":"02_getting_started/#etccvmfsdefaultlocal","text":"This file can be used for setting or overriding settings that are specific to your client machine. One required parameter is CVMFS_HTTP_PROXY , which should point to your local proxy that serves as a cache between your client(s) and the Stratum 1 server(s). Since we do not have a proxy yet, we are setting this to DIRECT , meaning that we connect directly to the Stratum 1: CVMFS_HTTP_PROXY=DIRECT You can also use this file to set a maximum size (in megabytes) for the cache, for instance: CVMFS_QUOTA_LIMIT=50000","title":"/etc/cvmfs/default.local"},{"location":"02_getting_started/#mount-the-repositories","text":"When your configuration is complete, you can run the following command as root to mount the repository: sudo cvmfs_config setup This should not return any error message.","title":"Mount the repositories"},{"location":"02_getting_started/#browse-the-repository","text":"Finally, we can try to access our repository on the client machine. Note that CernVM-FS uses autofs , which means that you may not see the repository when you do ls /cvmfs . Your repository will only be actually mounted when you access it, and may be unmounted after not using it for a while. So, the following should work and show the contents of your repository: ls /cvmfs/repo.organization.tld","title":"Browse the repository"},{"location":"02_getting_started/#exercise","text":"Set up your own Stratum 0 on a virtual machine. Create a repository with a suitable name ( name.domain.tld ). Add a simple bash script to the repository, which, for instance, prints Hello world! . Install and configure the CernVM-FS client on another virtual machine, using the Stratum 0 as your Stratum 1 for now. Try to access your repository and run your bash script on the client.","title":"Exercise"},{"location":"03_stratum1_proxies/","text":"Stratum 1 and proxies \u00b6 In the previous section we have set up a Stratum 0 server and a client that directly connects to the Stratum 0. Although this worked fine, this setup is neither scalable, nor very reliable, nor secure: it is a single point of failure, too open in terms of connectivity, and it will have to serve all possible clients on its own. Therefore, this section will show how all these points can be addressed by adding one or more Stratum 1 servers and caching proxy servers. A Stratum 1 is a replica server that keeps mirrors of the repositories served by a Stratum 0. It is basically a web server that periodically synchronizes the contents of the repositories, and, in contrast to a Stratum 0 server, you can have multiple Stratum 1 servers. It is recommended to have several ones that are geographically distributed, so that clients always have a nearby Stratum 1 server. How many you need mostly depends on the distribution and number of clients, but often a few is already sufficient. More scalability can be added with proxies, which we will discuss later in this section. INSERT IMAGE OF CVMFS INFRA HERE Set up a Stratum 1 server \u00b6 Requirements \u00b6 A Stratum 1 servers has similar requirements as a Stratum 1 in terms of resources. Regarding the storage, it could do with less, because the Stratum 1 stores a deduplicated and compressed version of the repositories. In addition to port 80, also port 8000 has to be accessible for a Stratum 1. Furthermore, you need a (free) license key for Maxmind's Geo API , which you can obtain by signing up for an account . Installation \u00b6 For the Stratum 1 you need to install the following packages: sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y epel-release sudo yum install -y cvmfs-server sudo yum install -y mod_wsgi sudo yum install -y squid Apache and Squid configuration \u00b6 We will be running Apache with a Squid frontend (reverse proxy); Apache will be listening internally on port 8080, while Squid needs to listen (externally) on port 80 and 8000, which are the default Stratum 1 ports. For this we first edit /etc/httpd/conf/httpd.conf and change the default: Listen 80 to: Listen 127.0.0.1:8080 Next, we replace the default contents of /etc/squid/squid.conf: with the following: http_port 80 accel http_port 8000 accel http_access allow all cache_peer 127.0.0.1 parent 8080 0 no-query originserver acl CVMFSAPI urlpath_regex ^/cvmfs/[^/]*/api/ cache deny !CVMFSAPI cache_mem 128 MB Finally, we start and enable Apache and Squid: sudo systemctl start httpd sudo systemctl start squid sudo systemctl enable httpd sudo systemctl enable squid DNS cache? \u00b6 Skip this for now and point to the documentation? Create the Stratum 1 replica \u00b6 With all the required components in place, we can now really set up our Stratum 1 replica server. We first add our Geo API key to the CernVM-FS server settings: echo 'CVMFS_GEO_LICENSE_KEY=YOUR_KEY' | sudo tee -a /etc/cvmfs/server.local sudo chmod 600 /etc/cvmfs/server.local We also need to have the public keys of all repositories we want to mirror to be available on our Stratum 1. This can be done by copying all the corresponding .pub files from /etc/cvmfs/keys on your Stratum 0 server to /etc/cvmfs/keys/organization.tld/ (note the extra level!) on your Stratum 1 server. Now we make the replica by giving the URL to the repository on the Stratum 0 server (which is always like http://host:port/cvmfs/repository ) and the path to the corresponding public key: sudo cvmfs_server add-replica -o $USER http://YOUR_STRATUM0/cvmfs/repo.organization.tld /etc/cvmfs/keys/organization.tld/repo.organization.tld Manually synchronize the Stratum 1 \u00b6 The Stratum 1 has been registered, so now we should try to do a first synchronization. You can do this by running the following command: sudo cvmfs_server snapshot repo.organization.tld As there is not much in the repository yet, this should complete within a few seconds. Make cronjobs \u00b6 Whenever you make changes to the repository, the changes have to be synchronized to all Stratum 1 servers. Furthermore, the Geo database has to be updated regularly. Both tasks can be automated by setting up cronjobs that periodically run cvmfs_server update-geodb and cvmfs_server snapshot -a , where -a does the synchronization for all active repositories. This option will give an error if no log rotation has been configured for CernVM-FS, so we first have to create a file /etc/logrotate.d/cvmfs with the following contents: /var/log/cvmfs/*.log { weekly missingok notifempty } Now we can make a cronjob /etc/cron.d/cvmfs_stratum1_snapshot for the snapshots: */5 * * * * root output=$(/usr/bin/cvmfs_server snapshot -a -i 2>&1) || echo \"$output\" And another cronjob /etc/cron.d/cvmfs_geoip_db_update for updating the Geo database: 4 2 2 * * root /usr/bin/cvmfs_server update-geodb Set up a proxy \u00b6 If you have a lot of local machines, e.g. a cluster, that need to access your repositories, you also want another cache layer close to these machines. This can be done by adding Squid proxies between your local machine(s) and the layer of Stratum 1 servers. Usually it is recommended to have at least two of them for reliability and load-balancing reasons. Requirements \u00b6 Just as with the other components, the squid proxy server does not need a lot of resources. Just a few cores and few gigabytes of memory should be enough. The more disk space you allocate for this machine, the larger the cache can be, and the better the performance will be. Do note that this machine will only store a part of the (deduplicated and compressed) repository, so it does not need as much space as your Stratum 1. Installation \u00b6 The proxy server only requires Squid to be installed: sudo yum install -y squid Configuration \u00b6 The configuration of a standalone Squid is slightly different from the one that we used for our Stratum 1. You can use the following template to set up your own configuration: # List of local IP addresses (separate IPs and/or CIDR notation) allowed to access your local proxy acl local_nodes src YOUR_CLIENT_IPS # Destination domains that are allowed #acl stratum_ones dstdomain .YOURDOMAIN.ORG #acl stratum_ones dstdom_regex YOUR_REGEX # Squid port (default: 3128) # http_port 3128 # Deny access to anything which is not part of our stratum_ones ACL. http_access deny !stratum_ones # Only allow access from our local machines http_access allow local_nodes http_access allow localhost # Finally, deny all other access to this proxy http_access deny all minimum_expiry_time 0 maximum_object_size 1024 MB cache_mem 128 MB maximum_object_size_in_memory 128 KB # 50 GB disk cache cache_dir ufs /var/spool/squid 50000 16 256 You should use the local_nodes ACL here to specify which clients are allowed to use this proxy; you can use CIDR notation . Furthermore, you probably also want to have an ACL that specifies that your Squid should only cache the Stratum 1 servers. The template uses a stratum_ones ACL for this, and you can make use of either dstdomain (in case you have a single domain for all your Stratum 1 servers) or dstdom_regex for more complex situations. More information about Squid ACLs can be found in the (Squid documentation)[http://www.squid-cache.org/Doc/config/acl/]. Finally, there are some settings regarding the size of your cache. Make sure that you have enough disk space for the value that you provide. Starting Squid \u00b6 Before you actually start Squid, you can verify the correctness of your configuration with sudo squid -k parse . When you are sure things are okay, start and enable Squid: sudo systemctl start squid sudo systemctl enable squid Client configuration \u00b6 Now that we have a Stratum 0, one ore more Stratum 1 servers, and one or more local Squid proxies, all the infrastructure for a production-ready CernVM-FS setup is in place. This means that we can now configure our client properly, and start using the repository. In the previous section we connected our client directly to the Stratum 0. We are going to reuse that configuration, and only need to change two things. Connect to the Stratum 1 \u00b6 We used the URL of the Stratum 0 in the file /etc/cvmfs/config.d/repo.organization.tld.conf . We should now change this URL, and point to the Stratum 1 instead: CVMFS_SERVER_URL=\"http://your-stratum1/cvmfs/@fqrn@\" When you have more Stratum 1 servers inside the organization, you can make it a semicolon-separated list of servers. The Geo API will make sure that your client always connects to the geographically closest Stratum 1 server. Use the Squid proxy \u00b6 In order to use the local cache layer of our proxy, we have to instruct the client to send all requests through the proxy. This needs one small change in /etc/cvmfs/default.local , where you will have to set: CVMFS_HTTP_PROXY=\"http://your-proxy:3128\" More proxies can be added to that list by separating them with a pipe symbol. See for more (complex) examples this documentation page . Homework \u00b6 Set up a Stratum 1 server. Make sure that it includes: a proper Geo API license key; cronjobs for automatically synchronizing the database and updating the Geo database; properly configured Apache and Squid services; Set up a separate Squid proxy. Though it is recommended to at least have two in production, one is enough for now. TODO: reuse or set up a new client?? Add firewall rules to the Stratum 0? \u00b6 Reconfigure the client that you set up in the previous section and make sure that it uses your Stratum 1 and Squid proxy.","title":"Stratum-1 + proxies"},{"location":"03_stratum1_proxies/#stratum-1-and-proxies","text":"In the previous section we have set up a Stratum 0 server and a client that directly connects to the Stratum 0. Although this worked fine, this setup is neither scalable, nor very reliable, nor secure: it is a single point of failure, too open in terms of connectivity, and it will have to serve all possible clients on its own. Therefore, this section will show how all these points can be addressed by adding one or more Stratum 1 servers and caching proxy servers. A Stratum 1 is a replica server that keeps mirrors of the repositories served by a Stratum 0. It is basically a web server that periodically synchronizes the contents of the repositories, and, in contrast to a Stratum 0 server, you can have multiple Stratum 1 servers. It is recommended to have several ones that are geographically distributed, so that clients always have a nearby Stratum 1 server. How many you need mostly depends on the distribution and number of clients, but often a few is already sufficient. More scalability can be added with proxies, which we will discuss later in this section. INSERT IMAGE OF CVMFS INFRA HERE","title":"Stratum 1 and proxies"},{"location":"03_stratum1_proxies/#set-up-a-stratum-1-server","text":"","title":"Set up a Stratum 1 server"},{"location":"03_stratum1_proxies/#requirements","text":"A Stratum 1 servers has similar requirements as a Stratum 1 in terms of resources. Regarding the storage, it could do with less, because the Stratum 1 stores a deduplicated and compressed version of the repositories. In addition to port 80, also port 8000 has to be accessible for a Stratum 1. Furthermore, you need a (free) license key for Maxmind's Geo API , which you can obtain by signing up for an account .","title":"Requirements"},{"location":"03_stratum1_proxies/#installation","text":"For the Stratum 1 you need to install the following packages: sudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm sudo yum install -y epel-release sudo yum install -y cvmfs-server sudo yum install -y mod_wsgi sudo yum install -y squid","title":"Installation"},{"location":"03_stratum1_proxies/#apache-and-squid-configuration","text":"We will be running Apache with a Squid frontend (reverse proxy); Apache will be listening internally on port 8080, while Squid needs to listen (externally) on port 80 and 8000, which are the default Stratum 1 ports. For this we first edit /etc/httpd/conf/httpd.conf and change the default: Listen 80 to: Listen 127.0.0.1:8080 Next, we replace the default contents of /etc/squid/squid.conf: with the following: http_port 80 accel http_port 8000 accel http_access allow all cache_peer 127.0.0.1 parent 8080 0 no-query originserver acl CVMFSAPI urlpath_regex ^/cvmfs/[^/]*/api/ cache deny !CVMFSAPI cache_mem 128 MB Finally, we start and enable Apache and Squid: sudo systemctl start httpd sudo systemctl start squid sudo systemctl enable httpd sudo systemctl enable squid","title":"Apache and Squid configuration"},{"location":"03_stratum1_proxies/#dns-cache","text":"Skip this for now and point to the documentation?","title":"DNS cache?"},{"location":"03_stratum1_proxies/#create-the-stratum-1-replica","text":"With all the required components in place, we can now really set up our Stratum 1 replica server. We first add our Geo API key to the CernVM-FS server settings: echo 'CVMFS_GEO_LICENSE_KEY=YOUR_KEY' | sudo tee -a /etc/cvmfs/server.local sudo chmod 600 /etc/cvmfs/server.local We also need to have the public keys of all repositories we want to mirror to be available on our Stratum 1. This can be done by copying all the corresponding .pub files from /etc/cvmfs/keys on your Stratum 0 server to /etc/cvmfs/keys/organization.tld/ (note the extra level!) on your Stratum 1 server. Now we make the replica by giving the URL to the repository on the Stratum 0 server (which is always like http://host:port/cvmfs/repository ) and the path to the corresponding public key: sudo cvmfs_server add-replica -o $USER http://YOUR_STRATUM0/cvmfs/repo.organization.tld /etc/cvmfs/keys/organization.tld/repo.organization.tld","title":"Create the Stratum 1 replica"},{"location":"03_stratum1_proxies/#manually-synchronize-the-stratum-1","text":"The Stratum 1 has been registered, so now we should try to do a first synchronization. You can do this by running the following command: sudo cvmfs_server snapshot repo.organization.tld As there is not much in the repository yet, this should complete within a few seconds.","title":"Manually synchronize the Stratum 1"},{"location":"03_stratum1_proxies/#make-cronjobs","text":"Whenever you make changes to the repository, the changes have to be synchronized to all Stratum 1 servers. Furthermore, the Geo database has to be updated regularly. Both tasks can be automated by setting up cronjobs that periodically run cvmfs_server update-geodb and cvmfs_server snapshot -a , where -a does the synchronization for all active repositories. This option will give an error if no log rotation has been configured for CernVM-FS, so we first have to create a file /etc/logrotate.d/cvmfs with the following contents: /var/log/cvmfs/*.log { weekly missingok notifempty } Now we can make a cronjob /etc/cron.d/cvmfs_stratum1_snapshot for the snapshots: */5 * * * * root output=$(/usr/bin/cvmfs_server snapshot -a -i 2>&1) || echo \"$output\" And another cronjob /etc/cron.d/cvmfs_geoip_db_update for updating the Geo database: 4 2 2 * * root /usr/bin/cvmfs_server update-geodb","title":"Make cronjobs"},{"location":"03_stratum1_proxies/#set-up-a-proxy","text":"If you have a lot of local machines, e.g. a cluster, that need to access your repositories, you also want another cache layer close to these machines. This can be done by adding Squid proxies between your local machine(s) and the layer of Stratum 1 servers. Usually it is recommended to have at least two of them for reliability and load-balancing reasons.","title":"Set up a proxy"},{"location":"03_stratum1_proxies/#requirements_1","text":"Just as with the other components, the squid proxy server does not need a lot of resources. Just a few cores and few gigabytes of memory should be enough. The more disk space you allocate for this machine, the larger the cache can be, and the better the performance will be. Do note that this machine will only store a part of the (deduplicated and compressed) repository, so it does not need as much space as your Stratum 1.","title":"Requirements"},{"location":"03_stratum1_proxies/#installation_1","text":"The proxy server only requires Squid to be installed: sudo yum install -y squid","title":"Installation"},{"location":"03_stratum1_proxies/#configuration","text":"The configuration of a standalone Squid is slightly different from the one that we used for our Stratum 1. You can use the following template to set up your own configuration: # List of local IP addresses (separate IPs and/or CIDR notation) allowed to access your local proxy acl local_nodes src YOUR_CLIENT_IPS # Destination domains that are allowed #acl stratum_ones dstdomain .YOURDOMAIN.ORG #acl stratum_ones dstdom_regex YOUR_REGEX # Squid port (default: 3128) # http_port 3128 # Deny access to anything which is not part of our stratum_ones ACL. http_access deny !stratum_ones # Only allow access from our local machines http_access allow local_nodes http_access allow localhost # Finally, deny all other access to this proxy http_access deny all minimum_expiry_time 0 maximum_object_size 1024 MB cache_mem 128 MB maximum_object_size_in_memory 128 KB # 50 GB disk cache cache_dir ufs /var/spool/squid 50000 16 256 You should use the local_nodes ACL here to specify which clients are allowed to use this proxy; you can use CIDR notation . Furthermore, you probably also want to have an ACL that specifies that your Squid should only cache the Stratum 1 servers. The template uses a stratum_ones ACL for this, and you can make use of either dstdomain (in case you have a single domain for all your Stratum 1 servers) or dstdom_regex for more complex situations. More information about Squid ACLs can be found in the (Squid documentation)[http://www.squid-cache.org/Doc/config/acl/]. Finally, there are some settings regarding the size of your cache. Make sure that you have enough disk space for the value that you provide.","title":"Configuration"},{"location":"03_stratum1_proxies/#starting-squid","text":"Before you actually start Squid, you can verify the correctness of your configuration with sudo squid -k parse . When you are sure things are okay, start and enable Squid: sudo systemctl start squid sudo systemctl enable squid","title":"Starting Squid"},{"location":"03_stratum1_proxies/#client-configuration","text":"Now that we have a Stratum 0, one ore more Stratum 1 servers, and one or more local Squid proxies, all the infrastructure for a production-ready CernVM-FS setup is in place. This means that we can now configure our client properly, and start using the repository. In the previous section we connected our client directly to the Stratum 0. We are going to reuse that configuration, and only need to change two things.","title":"Client configuration"},{"location":"03_stratum1_proxies/#connect-to-the-stratum-1","text":"We used the URL of the Stratum 0 in the file /etc/cvmfs/config.d/repo.organization.tld.conf . We should now change this URL, and point to the Stratum 1 instead: CVMFS_SERVER_URL=\"http://your-stratum1/cvmfs/@fqrn@\" When you have more Stratum 1 servers inside the organization, you can make it a semicolon-separated list of servers. The Geo API will make sure that your client always connects to the geographically closest Stratum 1 server.","title":"Connect to the Stratum 1"},{"location":"03_stratum1_proxies/#use-the-squid-proxy","text":"In order to use the local cache layer of our proxy, we have to instruct the client to send all requests through the proxy. This needs one small change in /etc/cvmfs/default.local , where you will have to set: CVMFS_HTTP_PROXY=\"http://your-proxy:3128\" More proxies can be added to that list by separating them with a pipe symbol. See for more (complex) examples this documentation page .","title":"Use the Squid proxy"},{"location":"03_stratum1_proxies/#homework","text":"Set up a Stratum 1 server. Make sure that it includes: a proper Geo API license key; cronjobs for automatically synchronizing the database and updating the Geo database; properly configured Apache and Squid services; Set up a separate Squid proxy. Though it is recommended to at least have two in production, one is enough for now.","title":"Homework"},{"location":"03_stratum1_proxies/#todo-reuse-or-set-up-a-new-client-add-firewall-rules-to-the-stratum-0","text":"Reconfigure the client that you set up in the previous section and make sure that it uses your Stratum 1 and Squid proxy.","title":"TODO: reuse or set up a new client?? Add firewall rules to the Stratum 0?"},{"location":"04_publishing/","text":"Publishing \u00b6 The previous sections were mostly about setting up the infrastructure. Now that all the components for hosting and accessing your own CernVM-FS repositories are in place, it is time to really start using it. In this section we will give some more details about publishing files. Transactions \u00b6 As we already showed in a previous section, the easiest way to add files to your repository is by opening and publishing a transaction on your Stratum 0 server. By default, your repository directory under /cvmfs is read-only, but by a transaction makes the directory writable for the user that is owner of the repository. sudo cvmfs_server transaction repo.organization.tld Once you are done with making changes, the changes can be published using: sudo cvmfs_server publish repo.organization.tld And you can always abort a transaction, which will undo all the non-published modifications: sudo cvmfs_server abort repo.organization.tld Ingesting tarballs \u00b6 When you need to compile software that you want to add to your repository, you may want to do the actual compilation on a different machine than your Stratum 0 and copy the resulting installation as a tarball to your Stratum 0. Instead of manually extracting the tarball and doing a transaction, the cvmfs_server command offers a more efficient method for directly publishing the contents of a tarball: sudo cvmfs_server ingest -b /some/path repo.organization.tld -t mytarball.tar The -b expects the relative location in your repository where the contents of the tarball, specified with -t , will be extracted. So, in this case, the tarball gets extracted to /cvmfs/repo.organization.tld/some/path . Note that passing / to -b does not work at the moment, but will be supported in a future release of CernVM-FS. In case you have a compressed tarball, you can use an appropriate decompression tool and write the output to stdout . This output can then be piped to cvmfs_server by setting -t - , e.g. for a .tar.gz file: gunzip -c mytarball.tar.gz | sudo cvmfs_server ingest -b /some/path -t - Tags \u00b6 By default, a newly published version of the repository will automatically get a tag with a timestamp in its name. This allows you to revert back to earlier versions. You can also set your own tag name and/or description upon publication: sudo cvmfs_server publish [-a tag name] [-m tag description] repo.organization.tld The tag subcommand for cvmfs_server allows you to create ( -a ), remove ( -r ), inspect ( -i ), or list ( -l ) tags of your repository, e.g.: sudo cvmfs_server tag -a \"v1.0\" repo.organization.tld sudo cvmfs_server tag -l repo.organization.tld With the rollback subcommand you can revert back to an earlier version. By default, this will be the previous version, but with -t you can specify a specific tag to revert to: sudo cvmfs_server rollback -t \"v0.5\" repo.organization.tld Catalogs \u00b6 All metadata about files in your repository is stored in a file catalog, which is a SQLite database. When a client accesses the repository for the first time, it first needs to retrieve this catalog. Only then it can start fetching the files it actually needs. Clients also need to regularly check for new versions of the repository, and redownload it if it has changed. As this catalog can quickly become quite large when you start adding more and more files, just having a single one would cause significant overhead. In order to keep them small, you can make use of nested catalogs by having several catalogs for different subtrees of your repository. All metadata for that part of the subtree will not be part of the main catalog anymore, and clients will only download the catalogs for the subtree(s) they are trying to access. The general recommendation is to have more than 1000 and fewer than 200,000 files/directories per (nested) catalog, and to bundle the files/directories that are often accessed together. For instance, it may make sense to make a catalog per installation directory of a specific version of some software in your repository. Making nested catalogs manually can be done in two ways, which we will describe in the following subsections. Note that you can also combine both methods. .cvmfscatalog files \u00b6 By adding an (empty) file named .cvmfscatalog into a directory of your repository, each following publish operation will automatically generate a nested catalog for the entire subtree below that directory. You can put these files at as many levels as you like, but do keep the aforementioned recommendations in mind. .cvmfsdirtab \u00b6 Instead of creating the .cvmfscatalog files manually, you can also add a file named .cvmfsdirtab to the root of your repository. In this file you can specify a list of relative directory paths (they all start from the root of your repository) that should get a nested catalog, and you can use wildcards to make things easier. For instance, assume you have a typical HPC software module environment in your repository: / \u251c\u2500 /software \u2502 \u251c\u2500 /software/app1 \u2502 \u2502 \u251c\u2500 /software/app1/1.0 \u2502 \u2502 \u251c\u2500 /software/app1/2.0 \u2502 \u251c\u2500 /software/app2 \u2502 \u2502 \u251c\u2500 /software/app2/20201201 \u2502 \u2502 \u251c\u2500 /software/app2/20210125 \u251c\u2500 /modules \u2502 \u251c\u2500 /modules/all \u2502 \u2502 \u251c\u2500 /modules/all/app1 \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app1/1.0.lua \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app1/2.0.lua \u2502 \u2502 \u251c\u2500 /modules/all/app2 \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app2/20201201.lua \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app2/20210125.lua For this structure, the .cvmfsdirtab may look like: # Nested catalog for each version of each application /software/*/* # Nested catalog containing for all modulefiles /modules After you have added this file to your repository, you should see automatically generated .cvmfscatalog files in all the specified directories. You can also run cvmfs_server list-catalogs to get a full list of all the nested catalogs. Homework \u00b6 We prepared a tarball that contains a tree with dummy software installations. You can find the tarball at: TODO: INSERT DETAILS Insert this tarball to a directory named software in your repository using the ingest subcommand; Note that you get some warnings about the catalog containing too many entries; Fix the catalog issue by adding a .cvmfsdirtab file to the root of your repo, which automatically makes a catalog for each software installation directory; Make sure that the warning is gone when you publish this .cvmfsdirtab file. Instead, you may see a message about the catalog being defragmented (because lots of entries were cleaned up).","title":"Publishing"},{"location":"04_publishing/#publishing","text":"The previous sections were mostly about setting up the infrastructure. Now that all the components for hosting and accessing your own CernVM-FS repositories are in place, it is time to really start using it. In this section we will give some more details about publishing files.","title":"Publishing"},{"location":"04_publishing/#transactions","text":"As we already showed in a previous section, the easiest way to add files to your repository is by opening and publishing a transaction on your Stratum 0 server. By default, your repository directory under /cvmfs is read-only, but by a transaction makes the directory writable for the user that is owner of the repository. sudo cvmfs_server transaction repo.organization.tld Once you are done with making changes, the changes can be published using: sudo cvmfs_server publish repo.organization.tld And you can always abort a transaction, which will undo all the non-published modifications: sudo cvmfs_server abort repo.organization.tld","title":"Transactions"},{"location":"04_publishing/#ingesting-tarballs","text":"When you need to compile software that you want to add to your repository, you may want to do the actual compilation on a different machine than your Stratum 0 and copy the resulting installation as a tarball to your Stratum 0. Instead of manually extracting the tarball and doing a transaction, the cvmfs_server command offers a more efficient method for directly publishing the contents of a tarball: sudo cvmfs_server ingest -b /some/path repo.organization.tld -t mytarball.tar The -b expects the relative location in your repository where the contents of the tarball, specified with -t , will be extracted. So, in this case, the tarball gets extracted to /cvmfs/repo.organization.tld/some/path . Note that passing / to -b does not work at the moment, but will be supported in a future release of CernVM-FS. In case you have a compressed tarball, you can use an appropriate decompression tool and write the output to stdout . This output can then be piped to cvmfs_server by setting -t - , e.g. for a .tar.gz file: gunzip -c mytarball.tar.gz | sudo cvmfs_server ingest -b /some/path -t -","title":"Ingesting tarballs"},{"location":"04_publishing/#tags","text":"By default, a newly published version of the repository will automatically get a tag with a timestamp in its name. This allows you to revert back to earlier versions. You can also set your own tag name and/or description upon publication: sudo cvmfs_server publish [-a tag name] [-m tag description] repo.organization.tld The tag subcommand for cvmfs_server allows you to create ( -a ), remove ( -r ), inspect ( -i ), or list ( -l ) tags of your repository, e.g.: sudo cvmfs_server tag -a \"v1.0\" repo.organization.tld sudo cvmfs_server tag -l repo.organization.tld With the rollback subcommand you can revert back to an earlier version. By default, this will be the previous version, but with -t you can specify a specific tag to revert to: sudo cvmfs_server rollback -t \"v0.5\" repo.organization.tld","title":"Tags"},{"location":"04_publishing/#catalogs","text":"All metadata about files in your repository is stored in a file catalog, which is a SQLite database. When a client accesses the repository for the first time, it first needs to retrieve this catalog. Only then it can start fetching the files it actually needs. Clients also need to regularly check for new versions of the repository, and redownload it if it has changed. As this catalog can quickly become quite large when you start adding more and more files, just having a single one would cause significant overhead. In order to keep them small, you can make use of nested catalogs by having several catalogs for different subtrees of your repository. All metadata for that part of the subtree will not be part of the main catalog anymore, and clients will only download the catalogs for the subtree(s) they are trying to access. The general recommendation is to have more than 1000 and fewer than 200,000 files/directories per (nested) catalog, and to bundle the files/directories that are often accessed together. For instance, it may make sense to make a catalog per installation directory of a specific version of some software in your repository. Making nested catalogs manually can be done in two ways, which we will describe in the following subsections. Note that you can also combine both methods.","title":"Catalogs"},{"location":"04_publishing/#cvmfscatalog-files","text":"By adding an (empty) file named .cvmfscatalog into a directory of your repository, each following publish operation will automatically generate a nested catalog for the entire subtree below that directory. You can put these files at as many levels as you like, but do keep the aforementioned recommendations in mind.","title":".cvmfscatalog files"},{"location":"04_publishing/#cvmfsdirtab","text":"Instead of creating the .cvmfscatalog files manually, you can also add a file named .cvmfsdirtab to the root of your repository. In this file you can specify a list of relative directory paths (they all start from the root of your repository) that should get a nested catalog, and you can use wildcards to make things easier. For instance, assume you have a typical HPC software module environment in your repository: / \u251c\u2500 /software \u2502 \u251c\u2500 /software/app1 \u2502 \u2502 \u251c\u2500 /software/app1/1.0 \u2502 \u2502 \u251c\u2500 /software/app1/2.0 \u2502 \u251c\u2500 /software/app2 \u2502 \u2502 \u251c\u2500 /software/app2/20201201 \u2502 \u2502 \u251c\u2500 /software/app2/20210125 \u251c\u2500 /modules \u2502 \u251c\u2500 /modules/all \u2502 \u2502 \u251c\u2500 /modules/all/app1 \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app1/1.0.lua \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app1/2.0.lua \u2502 \u2502 \u251c\u2500 /modules/all/app2 \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app2/20201201.lua \u2502 \u2502 \u2502 \u251c\u2500 /modules/all/app2/20210125.lua For this structure, the .cvmfsdirtab may look like: # Nested catalog for each version of each application /software/*/* # Nested catalog containing for all modulefiles /modules After you have added this file to your repository, you should see automatically generated .cvmfscatalog files in all the specified directories. You can also run cvmfs_server list-catalogs to get a full list of all the nested catalogs.","title":".cvmfsdirtab"},{"location":"04_publishing/#homework","text":"We prepared a tarball that contains a tree with dummy software installations. You can find the tarball at: TODO: INSERT DETAILS Insert this tarball to a directory named software in your repository using the ingest subcommand; Note that you get some warnings about the catalog containing too many entries; Fix the catalog issue by adding a .cvmfsdirtab file to the root of your repo, which automatically makes a catalog for each software installation directory; Make sure that the warning is gone when you publish this .cvmfsdirtab file. Instead, you may see a message about the catalog being defragmented (because lots of entries were cleaned up).","title":"Homework"},{"location":"05_advanced/","text":"Advanced topics \u00b6 gateway-publisher (with warnings) installing these takes time prepared: 1 gateway + 2 publishers demo only (no hands-on) exploding containers + use via Singularity demo","title":"Advanced topics"},{"location":"05_advanced/#advanced-topics","text":"gateway-publisher (with warnings) installing these takes time prepared: 1 gateway + 2 publishers demo only (no hands-on) exploding containers + use via Singularity demo","title":"Advanced topics"}]}